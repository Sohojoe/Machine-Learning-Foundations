{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Week 2: Multiple Regression (Interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this first notebook is to explore multiple regression and feature engineering with existing graphlab functions.\n",
    "\n",
    "In this notebook you will use data on house sales in King County to predict prices using multiple regression. You will:\n",
    "* Use SFrames to do some feature engineering\n",
    "* Use built-in graphlab functions to compute the regression weights (coefficients/parameters)\n",
    "* Given the regression weights, predictors and outcome write a function to compute the Residual Sum of Squares\n",
    "* Look at coefficients and interpret their meanings\n",
    "* Evaluate multiple models via RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire up graphlab create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in house sales data\n",
    "\n",
    "Dataset is from house sales in King County, the region where the city of Seattle, WA is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] This non-commercial license of GraphLab Create is assigned to joe@joebooth.com and will expire on October 09, 2016. For commercial licensing options, visit https://dato.com/buy/.\n",
      "\n",
      "[INFO] Start server at: ipc:///tmp/graphlab_server-29751 - Server binary: /Users/joebooth/.graphlab/anaconda/lib/python2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1449469167.log\n",
      "[INFO] GraphLab Server Version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "sales = graphlab.SFrame('kc_house_data.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#add 4 new vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales['bedrooms_squared'] = sales['bedrooms'] * sales['bedrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales['bed_bath_rooms'] = sales['bedrooms'] * sales['bathrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales['log_sqft_living'] = sales.apply(lambda x: math.log(x['sqft_living']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales['lat_plus_long'] = sales['lat'] * sales['long']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and testing.\n",
    "We use seed=0 so that everyone running this notebook gets the same results.  In practice, you may set a random seed (or let GraphLab Create pick a random seed for you).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data,test_data = sales.random_split(.8,seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#week 2 quiz 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.446677701584296"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['bedrooms_squared'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.503901631591392"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['bed_bath_rooms'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.55027467964594"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['log_sqft_living'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5812.99393545317"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['lat_plus_long'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function create in module graphlab.toolkits.regression.linear_regression:\n",
      "\n",
      "create(dataset, target, features=None, l2_penalty=0.01, l1_penalty=0.0, solver='auto', feature_rescaling=True, convergence_threshold=0.01, step_size=1.0, lbfgs_memory_level=11, max_iterations=10, validation_set='auto', verbose=True)\n",
      "    Create a :class:`~graphlab.linear_regression.LinearRegression` to\n",
      "    predict a scalar target variable as a linear function of one or more\n",
      "    features. In addition to standard numeric and categorical types, features\n",
      "    can also be extracted automatically from list- or dictionary-type SFrame\n",
      "    columns.\n",
      "    \n",
      "    The linear regression module can be used for ridge regression, Lasso, and\n",
      "    elastic net regression (see References for more detail on these methods). By\n",
      "    default, this model has an l2 regularization weight of 0.01.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    dataset : SFrame\n",
      "        The dataset to use for training the model.\n",
      "    \n",
      "    target : string\n",
      "        Name of the column containing the target variable.\n",
      "    \n",
      "    features : list[string], optional\n",
      "        Names of the columns containing features. 'None' (the default) indicates\n",
      "        that all columns except the target variable should be used as features.\n",
      "    \n",
      "        The features are columns in the input SFrame that can be of the\n",
      "        following types:\n",
      "    \n",
      "        - *Numeric*: values of numeric type integer or float.\n",
      "    \n",
      "        - *Categorical*: values of type string.\n",
      "    \n",
      "        - *Array*: list of numeric (integer or float) values. Each list element\n",
      "          is treated as a separate feature in the model.\n",
      "    \n",
      "        - *Dictionary*: key-value pairs with numeric (integer or float) values\n",
      "          Each key of a dictionary is treated as a separate feature and the\n",
      "          value in the dictionary corresponds to the value of the feature.\n",
      "          Dictionaries are ideal for representing sparse data.\n",
      "    \n",
      "        Columns of type *list* are not supported. Convert such feature\n",
      "        columns to type array if all entries in the list are of numeric\n",
      "        types. If the lists contain data of mixed types, separate\n",
      "        them out into different columns.\n",
      "    \n",
      "    l2_penalty : float, optional\n",
      "        Weight on the l2-regularizer of the model. The larger this weight, the\n",
      "        more the model coefficients shrink toward 0. This introduces bias into\n",
      "        the model but decreases variance, potentially leading to better\n",
      "        predictions. The default value is 0.01; setting this parameter to 0\n",
      "        corresponds to unregularized linear regression. See the ridge\n",
      "        regression reference for more detail.\n",
      "    \n",
      "    l1_penalty : float, optional\n",
      "        Weight on l1 regularization of the model. Like the l2 penalty, the\n",
      "        higher the l1 penalty, the more the estimated coefficients shrink toward\n",
      "        0. The l1 penalty, however, completely zeros out sufficiently small\n",
      "        coefficients, automatically indicating features that are not useful for\n",
      "        the model. The default weight of 0 prevents any features from being\n",
      "        discarded. See the LASSO regression reference for more detail.\n",
      "    \n",
      "    solver : string, optional\n",
      "        Solver to use for training the model. See the references for more detail\n",
      "        on each solver.\n",
      "    \n",
      "        - *auto (default)*: automatically chooses the best solver for the data\n",
      "          and model parameters.\n",
      "        - *newton*: Newton-Raphson\n",
      "        - *lbfgs*: limited memory BFGS\n",
      "        - *gd*: gradient descent\n",
      "        - *fista*: accelerated gradient descent\n",
      "    \n",
      "        The model is trained using a carefully engineered collection of methods\n",
      "        that are automatically picked based on the input data. The ``newton``\n",
      "        method  works best for datasets with plenty of examples and few features\n",
      "        (long datasets). Limited memory BFGS (``lbfgs``) is a robust solver for\n",
      "        wide datasets (i.e datasets with many coefficients).  ``fista`` is the\n",
      "        default solver for l1-regularized linear regression. Gradient-descent\n",
      "        (GD) is another well tuned method that can work really well on\n",
      "        l1-regularized problems.  The solvers are all automatically tuned and\n",
      "        the default options should function well. See the solver options guide\n",
      "        for setting additional parameters for each of the solvers.\n",
      "    \n",
      "    feature_rescaling : boolean, optional\n",
      "        Feature rescaling is an important pre-processing step that ensures that\n",
      "        all features are on the same scale. An l2-norm rescaling is performed\n",
      "        to make sure that all features are of the same norm. Categorical\n",
      "        features are also rescaled by rescaling the dummy variables that are\n",
      "        used to represent them. The coefficients are returned in original scale\n",
      "        of the problem. This process is particularly useful when features\n",
      "        vary widely in their ranges.\n",
      "    \n",
      "    validation_set : SFrame, optional\n",
      "    \n",
      "        A dataset for monitoring the model's generalization performance.\n",
      "        For each row of the progress table, the chosen metrics are computed\n",
      "        for both the provided training dataset and the validation_set. The\n",
      "        format of this SFrame must be the same as the training set.\n",
      "        By default this argument is set to 'auto' and a validation set is\n",
      "        automatically sampled and used for progress printing. If\n",
      "        validation_set is set to None, then no additional metrics\n",
      "        are computed. The default value is 'auto'.\n",
      "    \n",
      "    convergence_threshold : float, optional\n",
      "    \n",
      "      Convergence is tested using variation in the training objective. The\n",
      "      variation in the training objective is calculated using the difference\n",
      "      between the objective values between two steps. Consider reducing this\n",
      "      below the default value (0.01) for a more accurately trained model.\n",
      "      Beware of overfitting (i.e a model that works well only on the training\n",
      "      data) if this parameter is set to a very low value.\n",
      "    \n",
      "    lbfgs_memory_level : int, optional\n",
      "    \n",
      "      The L-BFGS algorithm keeps track of gradient information from the\n",
      "      previous ``lbfgs_memory_level`` iterations. The storage requirement for\n",
      "      each of these gradients is the ``num_coefficients`` in the problem.\n",
      "      Increasing the ``lbfgs_memory_level`` can help improve the quality of\n",
      "      the model trained. Setting this to more than ``max_iterations`` has the\n",
      "      same effect as setting it to ``max_iterations``.\n",
      "    \n",
      "    max_iterations : int, optional\n",
      "    \n",
      "      The maximum number of allowed passes through the data. More passes over\n",
      "      the data can result in a more accurately trained model. Consider\n",
      "      increasing this (the default value is 10) if the training accuracy is\n",
      "      low and the *Grad-Norm* in the display is large.\n",
      "    \n",
      "    step_size : float, optional (fista only)\n",
      "    \n",
      "      The starting step size to use for the ``fista`` and ``gd`` solvers. The\n",
      "      default is set to 1.0, this is an aggressive setting. If the first\n",
      "      iteration takes a considerable amount of time, reducing this parameter\n",
      "      may speed up model training.\n",
      "    \n",
      "    verbose : bool, optional\n",
      "        If True, print progress updates.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : LinearRegression\n",
      "        A trained model of type\n",
      "        :class:`~graphlab.linear_regression.LinearRegression`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    LinearRegression, graphlab.boosted_trees_regression.BoostedTreesRegression, graphlab.regression.create\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    - Categorical variables are encoded by creating dummy variables. For a\n",
      "      variable with :math:`K` categories, the encoding creates :math:`K-1` dummy\n",
      "      variables, while the first category encountered in the data is used as the\n",
      "      baseline.\n",
      "    \n",
      "    - For prediction and evaluation of linear regression models with sparse\n",
      "      dictionary inputs, new keys/columns that were not seen during training\n",
      "      are silently ignored.\n",
      "    \n",
      "    - Any 'None' values in the data will result in an error being thrown.\n",
      "    \n",
      "    - A constant term is automatically added for the model intercept. This term\n",
      "      is not regularized.\n",
      "    \n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    - Hoerl, A.E. and Kennard, R.W. (1970) `Ridge regression: Biased Estimation\n",
      "      for Nonorthogonal Problems\n",
      "      <http://amstat.tandfonline.com/doi/abs/10.1080/00401706.1970.10488634>`_.\n",
      "      Technometrics 12(1) pp.55-67\n",
      "    \n",
      "    - Tibshirani, R. (1996) `Regression Shrinkage and Selection via the Lasso <h\n",
      "      ttp://www.jstor.org/discover/10.2307/2346178?uid=3739256&uid=2&uid=4&sid=2\n",
      "      1104169934983>`_. Journal of the Royal Statistical Society. Series B\n",
      "      (Methodological) 58(1) pp.267-288.\n",
      "    \n",
      "    - Zhu, C., et al. (1997) `Algorithm 778: L-BFGS-B: Fortran subroutines for\n",
      "      large-scale bound-constrained optimization\n",
      "      <http://dl.acm.org/citation.cfm?id=279236>`_. ACM Transactions on\n",
      "      Mathematical Software 23(4) pp.550-560.\n",
      "    \n",
      "    - Barzilai, J. and Borwein, J. `Two-Point Step Size Gradient Methods\n",
      "      <http://imajna.oxfordjournals.org/content/8/1/141.short>`_. IMA Journal of\n",
      "      Numerical Analysis 8(1) pp.141-148.\n",
      "    \n",
      "    - Beck, A. and Teboulle, M. (2009) `A Fast Iterative Shrinkage-Thresholding\n",
      "      Algorithm for Linear Inverse Problems\n",
      "      <http://epubs.siam.org/doi/abs/10.1137/080716542>`_. SIAM Journal on\n",
      "      Imaging Sciences 2(1) pp.183-202.\n",
      "    \n",
      "    - Zhang, T. (2004) `Solving large scale linear prediction problems using\n",
      "      stochastic gradient descent algorithms\n",
      "      <http://dl.acm.org/citation.cfm?id=1015332>`_. ICML '04: Proceedings of\n",
      "      the twenty-first international conference on Machine learning p.116.\n",
      "    \n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    Given an :class:`~graphlab.SFrame` ``sf`` with a list of columns\n",
      "    [``feature_1`` ... ``feature_K``] denoting features and a target column\n",
      "    ``target``, we can create a\n",
      "    :class:`~graphlab.linear_regression.LinearRegression` as follows:\n",
      "    \n",
      "    >>> data =  graphlab.SFrame('http://s3.amazonaws.com/dato-datasets/regression/houses.csv')\n",
      "    \n",
      "    >>> model = graphlab.linear_regression.create(data, target='price',\n",
      "    ...                                  features=['bath', 'bedroom', 'size'])\n",
      "    \n",
      "    \n",
      "    For ridge regression, we can set the ``l2_penalty`` parameter higher (the\n",
      "    default is 0.01). For Lasso regression, we set the l1_penalty higher, and\n",
      "    for elastic net, we set both to be higher.\n",
      "    \n",
      "    .. sourcecode:: python\n",
      "    \n",
      "      # Ridge regression\n",
      "      >>> model_ridge = graphlab.linear_regression.create(data, 'price', l2_penalty=0.1)\n",
      "    \n",
      "      # Lasso\n",
      "      >>> model_lasso = graphlab.linear_regression.create(data, 'price', l2_penalty=0.,\n",
      "                                                                   l1_penalty=1.0)\n",
      "    \n",
      "      # Elastic net regression\n",
      "      >>> model_enet  = graphlab.linear_regression.create(data, 'price', l2_penalty=0.5,\n",
      "                                                                 l1_penalty=0.5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(graphlab.linear_regression.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-56-56e00ced8cd3>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-56e00ced8cd3>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ...                                  features=['sqft_living', 'bedrooms', 'bathrooms', 'lat', 'long'])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model1 = graphlab.linear_regression.create(test_data, target='price',\n",
    "    ...                                  features=['sqft_living', 'bedrooms', 'bathrooms', 'lat', 'long'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a multiple regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall we can use the following code to learn a multiple regression model predicting 'price' based on the following features:\n",
    "example_features = ['sqft_living', 'bedrooms', 'bathrooms'] on training data with the following code:\n",
    "\n",
    "(Aside: We set validation_set = None to ensure that the results are always the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Linear regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 17384\n",
      "PROGRESS: Number of features          : 3\n",
      "PROGRESS: Number of unpacked features : 3\n",
      "PROGRESS: Number of coefficients    : 4\n",
      "PROGRESS: Starting Newton Method\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 2        | 1.031675     | 4146407.600631     | 258679.804477 |\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "example_features = ['sqft_living', 'bedrooms', 'bathrooms']\n",
    "example_model = graphlab.linear_regression.create(train_data, target = 'price', features = example_features, \n",
    "                                                  validation_set = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted the model we can extract the regression weights (coefficients) as an SFrame as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+\n",
      "|     name    | index |     value      |\n",
      "+-------------+-------+----------------+\n",
      "| (intercept) |  None | 87910.0724924  |\n",
      "| sqft_living |  None | 315.403440552  |\n",
      "|   bedrooms  |  None | -65080.2155528 |\n",
      "|  bathrooms  |  None | 6944.02019265  |\n",
      "+-------------+-------+----------------+\n",
      "[4 rows x 3 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_weight_summary = example_model.get(\"coefficients\")\n",
    "print example_weight_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "\n",
    "In the gradient descent notebook we use numpy to do our regression. In this book we will use existing graphlab create functions to analyze multiple regressions. \n",
    "\n",
    "Recall that once a model is built we can use the .predict() function to find the predicted values for data we pass. For example using the example model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271789.505878\n"
     ]
    }
   ],
   "source": [
    "example_predictions = example_model.predict(train_data)\n",
    "print example_predictions[0] # should be 271789.505878"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can make predictions given the model, let's write a function to compute the RSS of the model. Complete the function below to calculate RSS given the model, data, and the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_residual_sum_of_squares(model, data, outcome):\n",
    "    # First get the predictions\n",
    "\n",
    "    # Then compute the residuals/errors\n",
    "\n",
    "    # Then square and add them up\n",
    "\n",
    "    return(RSS)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function by computing the RSS on TEST data for the example model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rss_example_train = get_residual_sum_of_squares(example_model, test_data, test_data['price'])\n",
    "print rss_example_train # should be 2.7376153833e+14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we often think of multiple regression as including multiple different features (e.g. # of bedrooms, squarefeet, and # of bathrooms) but we can also consider transformations of existing features e.g. the log of the squarefeet or even \"interaction\" features such as the product of bedrooms and bathrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the logarithm function to create a new feature. so first you should import it from the math library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create the following 4 new features as column in both TEST and TRAIN data:\n",
    "* bedrooms_squared = bedrooms\\*bedrooms\n",
    "* bed_bath_rooms = bedrooms\\*bathrooms\n",
    "* log_sqft_living = log(sqft_living)\n",
    "* lat_plus_long = lat + long \n",
    "As an example here's the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['bedrooms_squared'] = train_data['bedrooms'].apply(lambda x: x**2)\n",
    "test_data['bedrooms_squared'] = test_data['bedrooms'].apply(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the remaining 3 features in both TEST and TRAIN data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this feature will mostly affect houses with many bedrooms.\n",
    "* bedrooms times bathrooms gives what's called an \"interaction\" feature. It is large when *both* of them are large.\n",
    "* Taking the log of squarefeet has the effect of bringing large values closer together and spreading out small values.\n",
    "* Adding latitude to longitude is totally non-sensical but we will do it anyway (you'll see why)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: What is the mean (arithmetic average) value of your 4 new features on TEST data? (round to 2 digits)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Multiple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will learn the weights for three (nested) models for predicting house prices. The first model will have the fewest features the second model will add one more feature and the third will add a few more:\n",
    "* Model 1: squarefeet, # bedrooms, # bathrooms, latitude & longitude\n",
    "* Model 2: add bedrooms\\*bathrooms\n",
    "* Model 3: Add log squarefeet, bedrooms squared, and the (nonsensical) latitude + longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1_features = ['sqft_living', 'bedrooms', 'bathrooms', 'lat', 'long']\n",
    "model_2_features = model_1_features + ['bed_bath_rooms']\n",
    "model_3_features = model_2_features + ['bedrooms_squared', 'log_sqft_living', 'lat_plus_long']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the features, learn the weights for the three different models for predicting target = 'price' using graphlab.linear_regression.create() and look at the value of the weights/coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learn the three models: (don't forget to set validation_set = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examine/extract each model's coefficients:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: What is the sign (positive or negative) for the coefficient/weight for 'bathrooms' in model 1?**\n",
    "\n",
    "**Quiz Question: What is the sign (positive or negative) for the coefficient/weight for 'bathrooms' in model 2?**\n",
    "\n",
    "Think about what this means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing multiple models\n",
    "\n",
    "Now that you've learned three models and extracted the model weights we want to evaluate which model is best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use your functions from earlier to compute the RSS on TRAINING Data for each of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the RSS on TRAINING data for each of the three models and record the values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: Which model (1, 2 or 3) has lowest RSS on TRAINING Data?** Is this what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the RSS on on TEST data for each of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the RSS on TESTING data for each of the three models and record the values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: Which model (1, 2 or 3) has lowest RSS on TESTING Data?** Is this what you expected?Think about the features that were added to each model from the previous."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
